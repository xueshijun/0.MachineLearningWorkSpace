{
 "metadata": {
  "name": "",
  "signature": "sha256:998ad9049788feb599ec877afda11af2bf914b53cb207951f41badc2da6a0585"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "\u63d0\u53d6\u5fae\u683c\u5f0f\u3001\u63a8\u65ad\u8d44\u6e90\u53ca\u63cf\u8ff0\u6846\u67b6"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\u7b2c8\u7ae0 \u6316\u6398\u5e26\u6807\u8bb0\u8bed\u4e49\u7f51\uff1a\u63d0\u53d6\u5fae\u683c\u5f0f\u3001\u63a8\u65ad\u8d44\u6e90\u63cf\u8ff0\u6846\u67b6\u7b49 289\n",
      "8.1 \u6982\u8ff0 290\n",
      "8.2 \u5fae\u683c\u5f0f\uff1a\u6613\u4e8e\u5b9e\u73b0\u7684\u5143\u6570\u636e 290\n",
      "8.3 \u4ece\u8bed\u4e49\u6807\u8bb0\u8fc7\u6e21\u5230\u8bed\u4e49\u7f51\uff1a\u4e00\u4e2a\u5c0f\u63d2\u66f2 304\n",
      "8.4 \u8bed\u4e49\u7f51\uff1a\u53d1\u5c55\u4e2d\u7684\u53d8\u9769 304\n",
      "8.5 \u672c\u7ae0\u5c0f\u7ed3 310\n",
      "8.6 \u63a8\u8350\u7684\u7ec3\u4e60 311\n",
      "8.7 \u5728\u7ebf\u8d44\u6e90 311"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Example 1. \u4eceWikipedia page\u62bd\u53d6geo\u5fae\u683c\u5f0f\u6570\u636e"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests # pip install requests\n",
      "from bs4 import BeautifulSoup # pip install BeautifulSoup\n",
      "\n",
      "# XXX: Any URL containing a geo microformat...\n",
      "\n",
      "URL = 'http://en.wikipedia.org/wiki/Franklin,_Tennessee'\n",
      "\n",
      "# In the case of extracting content from Wikipedia, be sure to\n",
      "# review its \"Bot Policy,\" which is defined at\n",
      "# http://meta.wikimedia.org/wiki/Bot_policy#Unacceptable_usage\n",
      "\n",
      "req = requests.get(URL, headers={'User-Agent' : \"Mining the Social Web\"})\n",
      "soup = BeautifulSoup(req.text, \"lxml\")\n",
      "\n",
      "geoTag = soup.find(True, 'geo')\n",
      "\n",
      "if geoTag and len(geoTag) > 1:\n",
      "    lat = geoTag.find(True, 'latitude').string\n",
      "    lon = geoTag.find(True, 'longitude').string\n",
      "    print 'Location is at', lat, lon\n",
      "elif geoTag and len(geoTag) == 1:\n",
      "    (lat, lon) = geoTag.string.split(';')\n",
      "    (lat, lon) = (lat.strip(), lon.strip())\n",
      "    print 'Location is at', lat, lon\n",
      "else:\n",
      "    print 'No location found'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Location is at 35.92917 -86.85750\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Example 2.\u7528IPython Notebook\u5728Google Maps\u663e\u793ageo\u5fae\u683c\u5f0f"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import IFrame\n",
      "from IPython.core.display import display\n",
      "\n",
      "# Google Maps URL template for an iframe\n",
      "#Google Static Maps API:MliYFl08GQsktR-UwHi1YPoRWyY=\n",
      "#AIzaSyCtMl7mO0lXYuYkN6Aj3LQSW6qXARBlMm0\n",
      "#google_maps_url = \"http://maps.google.com/maps?q={0}+{1}& ie=UTF8&t=h&z=14&{0},{1}&output=embed\".format(lat, lon) \n",
      "google_maps_url=\"https://maps.googleapis.com/maps/api/staticmap?center=40.714,-73.998&zoom=12&size=400x400&key=AIzaSyCtMl7mO0lXYuYkN6Aj3LQSW6qXARBlMm0\"\n",
      "#google_maps_url=\"http://maps.googleapis.com/maps/api/staticmap?center={0},{1}&zoom=14&size=400x300&sensor=false\".format(lat, lon);\n",
      "\n",
      "display(IFrame(google_maps_url, '425px', '350px'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "        <iframe\n",
        "            width=\"425px\"\n",
        "            height=350px\"\n",
        "            src=\"https://maps.googleapis.com/maps/api/staticmap?center=40.714,-73.998&zoom=12&size=400x400&key=AIzaSyCtMl7mO0lXYuYkN6Aj3LQSW6qXARBlMm0\"\n",
        "            frameborder=\"0\"\n",
        "            allowfullscreen\n",
        "        ></iframe>\n",
        "        "
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.lib.display.IFrame at 0x4877770>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Example 3. \u4ece\u7f51\u9875\u4e2d\u63d0\u53d6hRecipe\u6570\u636e"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import requests\n",
      "import json\n",
      "from bs4 import BeautifulSoup # pip install BeautifulSoup\n",
      "\n",
      "# Pass in a URL containing hRecipe...\n",
      "\n",
      "URL = 'http://britishfood.about.com/od/recipeindex/r/applepie.htm'\n",
      "\n",
      "# Parse out some of the pertinent information for a recipe.\n",
      "# See http://microformats.org/wiki/hrecipe.\n",
      "\n",
      "\n",
      "def parse_hrecipe(url):\n",
      "    req = requests.get(URL)\n",
      "    \n",
      "    soup = BeautifulSoup(req.text,\"lxml\")\n",
      "    \n",
      "    hrecipe = soup.find(True, 'hrecipe')\n",
      "\n",
      "    if hrecipe and len(hrecipe) > 1:\n",
      "        fn = hrecipe.find(True, 'fn').string\n",
      "        author = hrecipe.find(True, 'author').find(text=True)\n",
      "        ingredients = [i.string for i in hrecipe.findAll(True, 'ingredient') if i.string is not None]\n",
      "\n",
      "        instructions = []\n",
      "        for i in hrecipe.find(True, 'instructions'):\n",
      "            if type(i) == BeautifulSoup.Tag:\n",
      "                s = ''.join(i.findAll(text=True)).strip()\n",
      "            elif type(i) == BeautifulSoup.NavigableString:\n",
      "                s = i.string.strip()\n",
      "            else:\n",
      "                continue\n",
      "\n",
      "            if s != '': \n",
      "                instructions += [s]\n",
      "\n",
      "        return {\n",
      "            'name': fn,\n",
      "            'author': author,\n",
      "            'ingredients': ingredients,\n",
      "            'instructions': instructions,\n",
      "            }\n",
      "    else:\n",
      "        return {}\n",
      "\n",
      "\n",
      "recipe = parse_hrecipe(URL)\n",
      "print json.dumps(recipe, indent=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{}\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Example 4. \u89e3\u6790\u4e00\u4e2a\u83dc\u8c31\u7684hReview-aggregate\u5fae\u683c\u5f0f\u6570\u636e"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "import json\n",
      "from bs4 import BeautifulSoup # pip install BeautifulSoup\n",
      "\n",
      "# Pass in a URL that contains hReview-aggregate info...\n",
      "\n",
      "URL = 'http://britishfood.about.com/od/recipeindex/r/applepie.htm'\n",
      "\n",
      "def parse_hreview_aggregate(url, item_type):\n",
      "    \n",
      "    req = requests.get(url)\n",
      "    print '--------------------------------'\n",
      "    soup = BeautifulSoup(req.text, \"lxml\")\n",
      "    print '--------------------------------'\n",
      "    \n",
      "    # Find the hRecipe or whatever other kind of parent item encapsulates\n",
      "    # the hReview (a required field).\n",
      "    \n",
      "    item_element = soup.find(item_type)\n",
      "    item = item_element.find('item').find('fn').text\n",
      "        \n",
      "    # And now parse out the hReview\n",
      "    hreview = soup.find('hreview-aggregate')\n",
      "    \n",
      "    # Required field\n",
      "    rating = hreview.find('rating').find('value-title')['title']\n",
      "    \n",
      "    # Optional fields\n",
      "    \n",
      "    try:\n",
      "        count = hreview.find('count').text\n",
      "    except AttributeError: # optional\n",
      "        count = None\n",
      "    try:\n",
      "        votes = hreview.find('votes').text\n",
      "    except AttributeError: # optional\n",
      "        votes = None\n",
      "\n",
      "    try:\n",
      "        summary = hreview.find('summary').text\n",
      "    except AttributeError: # optional\n",
      "        summary = None\n",
      "\n",
      "    return {\n",
      "        'item': item,\n",
      "        'rating': rating,\n",
      "        'count': count,\n",
      "        'votes': votes,\n",
      "        'summary' : summary\n",
      "    }\n",
      "\n",
      "# Find hReview aggregate information for an hRecipe\n",
      "\n",
      "reviews = parse_hreview_aggregate(URL, 'hrecipe')\n",
      "\n",
      "print json.dumps(reviews, indent=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "--------------------------------\n",
        "--------------------------------"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'NoneType' object has no attribute 'find'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-1cb47e829ff7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# Find hReview aggregate information for an hRecipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mreviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_hreview_aggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hrecipe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-2-1cb47e829ff7>\u001b[0m in \u001b[0;36mparse_hreview_aggregate\u001b[1;34m(url, item_type)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mitem_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# And now parse out the hReview\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note: You may also want to try Google's [structured data testing tool](http://www.google.com/webmasters/tools/richsnippets) to extract semantic markup from a webpage**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "**Note: You can use bash cell magic as shown below to invoke FuXi on the [sample data file](files/resources/ch08-semanticweb/chuck-norris.n3) introduced at the end of the chapter as follows:**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}